{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9247b87-9067-4a71-9a62-2ef1784b7db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# ImageNet transforms\n",
    "# ----------------------------\n",
    "imagenet_transform = T.Compose([\n",
    "    T.ToTensor(),  # H,W,C -> C,H,W\n",
    "    T.Resize((224, 224)),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# Training dataset: single sweep per sample\n",
    "# ----------------------------\n",
    "class SweepDataset(Dataset):\n",
    "    def __init__(self, csv_path, transform=None, load_nifti=True):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.transform = transform\n",
    "        self.load_nifti = load_nifti\n",
    "        self.sweep_cols = [c for c in self.df.columns if c.startswith('path_nifti')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = random.choice(row[self.sweep_cols])\n",
    "        \n",
    "        if self.load_nifti:\n",
    "            img = nib.load(path).get_fdata().astype(np.float32)  # W,H,C=1,Frames\n",
    "\n",
    "            # print(\"Load nifti\", img.shape)\n",
    "\n",
    "            # Get the number of frames\n",
    "            num_frames = img.shape[-1]\n",
    "            target_frames = 16\n",
    "            \n",
    "            if num_frames >= target_frames:\n",
    "                # Sample 16 frames uniformly\n",
    "                indices = np.linspace(0, num_frames - 1, target_frames, dtype=int)\n",
    "                sampled_img = img[..., indices]\n",
    "            else:\n",
    "                # Less than 16 frames, repeat frames to make 16\n",
    "                repeat_factor = int(np.ceil(target_frames / num_frames))\n",
    "                repeated_img = np.tile(img, (1, 1, 1, repeat_factor))  # repeat along frames\n",
    "                sampled_img = repeated_img[..., :target_frames]  # take exactly 16 frames\n",
    "            img = sampled_img\n",
    "        else:\n",
    "            img = path\n",
    "\n",
    "        # Use all frames\n",
    "        frames = []\n",
    "        for f in range(img.shape[-1]):\n",
    "            frame = np.repeat(img[:,:,:,f], 3, axis=2)  # 3 channels\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "        frames = torch.stack(frames, dim=0)  # (T, C, H, W)\n",
    "\n",
    "        label = torch.tensor(row['ga'], dtype=torch.float32)\n",
    "        return frames, label\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Validation/Test dataset: multiple sweeps per sample\n",
    "# ----------------------------\n",
    "class SweepEvalDataset(Dataset):\n",
    "    def __init__(self, csv_path, n_sweeps=None, transform=None, load_nifti=True):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.transform = transform\n",
    "        self.load_nifti = load_nifti\n",
    "        self.sweep_cols = [c for c in self.df.columns if c.startswith('path_nifti')]\n",
    "        self.n_sweeps = n_sweeps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        sweeps = row[self.sweep_cols].tolist()\n",
    "        if self.n_sweeps:\n",
    "            sweeps = sweeps[:self.n_sweeps]\n",
    "\n",
    "        all_sweeps = []\n",
    "        for path in sweeps:\n",
    "            if self.load_nifti:\n",
    "                img = nib.load(path).get_fdata().astype(np.float32)\n",
    "\n",
    "                target_frames = 16\n",
    "                num_frames = img.shape[-1]\n",
    "                if num_frames >= target_frames:\n",
    "                    # Sample 16 frames uniformly\n",
    "                    indices = np.linspace(0, num_frames - 1, target_frames, dtype=int)\n",
    "                    sampled_img = img[..., indices]\n",
    "                else:\n",
    "                    # Less than 16 frames, repeat frames to make 16\n",
    "                    repeat_factor = int(np.ceil(target_frames / num_frames))\n",
    "                    repeated_img = np.tile(img, (1, 1, 1, repeat_factor))  # repeat along frames\n",
    "                    sampled_img = repeated_img[..., :target_frames]  # take exactly 16 frames\n",
    "                img = sampled_img\n",
    "\n",
    "\n",
    "                \n",
    "                frames = []\n",
    "                for f in range(img.shape[-1]):\n",
    "                    frame = np.repeat(img[:,:,:,f], 3, axis=2)  # 3 channels\n",
    "                    if self.transform:\n",
    "                        frame = self.transform(frame)\n",
    "                    frames.append(frame)\n",
    "                frames = torch.stack(frames, dim=0)  # (T, C, H, W)\n",
    "            else:\n",
    "                frames = path\n",
    "            all_sweeps.append(frames)\n",
    "\n",
    "        all_sweeps = torch.stack(all_sweeps, dim=0)  # (num_sweeps, T, C, H, W)\n",
    "        label = torch.tensor(row['ga'], dtype=torch.float32)\n",
    "        return all_sweeps, label\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Model Definition\n",
    "# ----------------------------\n",
    "class WeightedAverageAttention(nn.Module):\n",
    "    def __init__(self, feature_dim=512, reduced_dim=128):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(feature_dim, 64)\n",
    "        self.V = nn.Linear(64, 1)\n",
    "        self.Q = nn.Linear(feature_dim, reduced_dim)\n",
    "\n",
    "    def forward(self, features):\n",
    "        attn_scores = self.V(torch.tanh(self.W(features)))  # (B,T,1)\n",
    "        attn_weights = F.softmax(attn_scores, dim=1)        # (B,T,1)\n",
    "        reduced_features = self.Q(features)                 # (B,T,reduced_dim)\n",
    "        weighted_sum = torch.sum(attn_weights * reduced_features, dim=1)\n",
    "        return weighted_sum, attn_weights.squeeze(-1)\n",
    "\n",
    "\n",
    "class NEJMbaseline(nn.Module):\n",
    "    def __init__(self, reduced_dim=128, fine_tune_backbone=True, pretrained=True):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=pretrained)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.feature_dim = 512\n",
    "        if not fine_tune_backbone:\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.attention = WeightedAverageAttention(feature_dim=self.feature_dim, reduced_dim=reduced_dim)\n",
    "        self.fc = nn.Linear(reduced_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, C, H, W)\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B*T, C, H, W)\n",
    "        features = self.feature_extractor(x)  # (B*T, 2048,1,1)\n",
    "        features = features.view(B, T, self.feature_dim)\n",
    "        aggregated, attn_weights = self.attention(features)\n",
    "        output = self.fc(aggregated)\n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Training + Validation\n",
    "# ----------------------------\n",
    "def train_and_validate(train_csv, val_csv, epochs=100, batch_size=8, n_sweeps_val=8, save_path = 'best_model.pth'):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_dataset = SweepDataset(train_csv, transform=imagenet_transform)\n",
    "    val_dataset = SweepEvalDataset(val_csv, n_sweeps=n_sweeps_val, transform=imagenet_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    model = NEJMbaseline().to(device)\n",
    "    criterion = nn.L1Loss()  # MAE\n",
    "    optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # ---------------- Training ----------------\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for frames, labels in train_loader:\n",
    "            frames = frames.to(device)  # (B, T, C, H, W)\n",
    "            labels = labels.float().to(device).unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * frames.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1} | Train MAE: {train_loss:.4f}\")\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # ---------------- Validation ----------------\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for sweeps, labels in val_loader:\n",
    "                sweeps = sweeps.to(device)  # (B, num_sweeps, T, C, H, W)\n",
    "                B, S, T, C, H, W = sweeps.shape\n",
    "                sweeps = sweeps.view(B, S*T, C, H, W)  # combine sweeps and frames\n",
    "                labels = labels.float().to(device).unsqueeze(1)\n",
    "                outputs, _ = model(sweeps)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * B\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1} | Val MAE: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "        # ---------------- Save best model ----------------\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(f\"✅ Saving new best model (Val MAE: {val_loss:.4f})\")\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'model_architecture': model,\n",
    "                'epoch': epoch + 1,\n",
    "                'val_loss': val_loss\n",
    "            }, save_path)\n",
    "\n",
    "    print(f\"Training completed. Best Val MAE: {best_val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf08c540-a59f-4dac-8733-f2406bcb4581",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_csv = '/mnt/Data/hackathon/data_splits/final_valid.csv'\n",
    "\n",
    "def infer_test(test_csv, model_path='best_model.pth', n_sweeps_test=8, output_csv='test_predictions.csv'):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load saved model\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only = False)\n",
    "    model = checkpoint['model_architecture']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare test data\n",
    "    test_df = pd.read_csv(test_csv)\n",
    "    test_dataset = SweepEvalDataset(test_csv, n_sweeps=n_sweeps_test, transform=imagenet_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "    predictions = []\n",
    "    study_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (sweeps, _) in enumerate(test_loader):\n",
    "            sweeps = sweeps.to(device)\n",
    "            B, S, T, C, H, W = sweeps.shape\n",
    "            sweeps = sweeps.view(B, S*T, C, H, W)\n",
    "            outputs, _ = model(sweeps)\n",
    "            preds = outputs.squeeze(1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "\n",
    "            start_idx = i * test_loader.batch_size\n",
    "            end_idx = min(start_idx + B, len(test_df))\n",
    "            study_ids.extend(test_df.iloc[start_idx:end_idx]['study_id'].tolist())\n",
    "\n",
    "    # Save predictions\n",
    "    result_df = pd.DataFrame({'study_id': study_ids, 'ga': predictions})\n",
    "    result_df.to_csv(output_csv, index=False)\n",
    "    print(f\"✅ Saved predictions to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0e4d086-c45e-40f2-9d60-076f9465a906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved predictions to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "test_csv = '/mnt/Data/hackathon/data_splits/final_test.csv'\n",
    "infer_test(test_csv, model_path='best_model.pth', n_sweeps_test=8, output_csv='predictions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff37f3b-b9d7-40b4-b415-36d2a41870fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Example usage\n",
    "# ----------------------------\n",
    "train_csv = '/mnt/Data/hackathon/data_splits/final_train.csv'\n",
    "val_csv = '/mnt/Data/hackathon/data_splits/final_valid.csv'\n",
    "\n",
    "train_and_validate(train_csv, val_csv, epochs=100, batch_size=16, n_sweeps_val=8, save_path='best_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cda240e4-34cc-478e-b26a-6badaf629cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>ga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KA-AC-190-1</td>\n",
       "      <td>59.716717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KA-AC-111-1</td>\n",
       "      <td>80.689354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KA-AC-107-1</td>\n",
       "      <td>65.882286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KA-AC-76-1</td>\n",
       "      <td>81.253654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KA-AD-68-1</td>\n",
       "      <td>73.344540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KA-AD-204-1</td>\n",
       "      <td>65.041410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KA-AC-213-1</td>\n",
       "      <td>77.429756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KA-AD-193-1</td>\n",
       "      <td>63.790897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KA-AD-134-1</td>\n",
       "      <td>73.665955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KA-AC-84-1</td>\n",
       "      <td>47.635925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PN-AD-7-1</td>\n",
       "      <td>76.417930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PN-AC-67-1</td>\n",
       "      <td>78.007770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PN-AD-60-1</td>\n",
       "      <td>69.703705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PN-AD-66-1</td>\n",
       "      <td>79.368350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PN-AC-13-1</td>\n",
       "      <td>70.050920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PN-AD-58-1</td>\n",
       "      <td>63.053120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PN-AC-25-1</td>\n",
       "      <td>72.088730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PN-AD-57-1</td>\n",
       "      <td>76.378845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PN-AC-124-1</td>\n",
       "      <td>71.979570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PN-AC-117-1</td>\n",
       "      <td>73.208510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NL-AC-106-1</td>\n",
       "      <td>50.712044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NL-AC-72-1</td>\n",
       "      <td>61.889305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NL-AC-3-1</td>\n",
       "      <td>44.847300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NL-AC-58-1</td>\n",
       "      <td>73.917984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NL-AC-29-1</td>\n",
       "      <td>79.514114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NL-AC-96-1</td>\n",
       "      <td>53.860493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NL-AF-52-1</td>\n",
       "      <td>72.350110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NL-AC-32-1</td>\n",
       "      <td>54.976190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NL-AC-30-1</td>\n",
       "      <td>74.697105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NL-AC-70-1</td>\n",
       "      <td>82.133930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NA-AE-11-1</td>\n",
       "      <td>64.400375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NA-AF-7-1</td>\n",
       "      <td>73.475784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NA-AF-16-1</td>\n",
       "      <td>84.932080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NA-AF-9-1</td>\n",
       "      <td>59.546190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NA-AF-10-1</td>\n",
       "      <td>70.188470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NA-AE-23-1</td>\n",
       "      <td>71.109760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NA-AF-66-1</td>\n",
       "      <td>60.984463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NA-AF-49-1</td>\n",
       "      <td>63.382366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NA-AF-15-1</td>\n",
       "      <td>59.787567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>NA-AE-33-1</td>\n",
       "      <td>78.967480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>UA-AF-29-1</td>\n",
       "      <td>84.218500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>UA-AC-66-1</td>\n",
       "      <td>61.847440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>UA-AC-35-1</td>\n",
       "      <td>80.119030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>UA-AC-15-1</td>\n",
       "      <td>61.296900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>UA-AF-19-1</td>\n",
       "      <td>78.842476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>UA-AC-61-1</td>\n",
       "      <td>74.707770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>UA-AC-65-1</td>\n",
       "      <td>73.300200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>UA-AD-86-1</td>\n",
       "      <td>68.761440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>UA-AF-81-1</td>\n",
       "      <td>67.195175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>UA-AF-17-1</td>\n",
       "      <td>57.880234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       study_id         ga\n",
       "0   KA-AC-190-1  59.716717\n",
       "1   KA-AC-111-1  80.689354\n",
       "2   KA-AC-107-1  65.882286\n",
       "3    KA-AC-76-1  81.253654\n",
       "4    KA-AD-68-1  73.344540\n",
       "5   KA-AD-204-1  65.041410\n",
       "6   KA-AC-213-1  77.429756\n",
       "7   KA-AD-193-1  63.790897\n",
       "8   KA-AD-134-1  73.665955\n",
       "9    KA-AC-84-1  47.635925\n",
       "10    PN-AD-7-1  76.417930\n",
       "11   PN-AC-67-1  78.007770\n",
       "12   PN-AD-60-1  69.703705\n",
       "13   PN-AD-66-1  79.368350\n",
       "14   PN-AC-13-1  70.050920\n",
       "15   PN-AD-58-1  63.053120\n",
       "16   PN-AC-25-1  72.088730\n",
       "17   PN-AD-57-1  76.378845\n",
       "18  PN-AC-124-1  71.979570\n",
       "19  PN-AC-117-1  73.208510\n",
       "20  NL-AC-106-1  50.712044\n",
       "21   NL-AC-72-1  61.889305\n",
       "22    NL-AC-3-1  44.847300\n",
       "23   NL-AC-58-1  73.917984\n",
       "24   NL-AC-29-1  79.514114\n",
       "25   NL-AC-96-1  53.860493\n",
       "26   NL-AF-52-1  72.350110\n",
       "27   NL-AC-32-1  54.976190\n",
       "28   NL-AC-30-1  74.697105\n",
       "29   NL-AC-70-1  82.133930\n",
       "30   NA-AE-11-1  64.400375\n",
       "31    NA-AF-7-1  73.475784\n",
       "32   NA-AF-16-1  84.932080\n",
       "33    NA-AF-9-1  59.546190\n",
       "34   NA-AF-10-1  70.188470\n",
       "35   NA-AE-23-1  71.109760\n",
       "36   NA-AF-66-1  60.984463\n",
       "37   NA-AF-49-1  63.382366\n",
       "38   NA-AF-15-1  59.787567\n",
       "39   NA-AE-33-1  78.967480\n",
       "40   UA-AF-29-1  84.218500\n",
       "41   UA-AC-66-1  61.847440\n",
       "42   UA-AC-35-1  80.119030\n",
       "43   UA-AC-15-1  61.296900\n",
       "44   UA-AF-19-1  78.842476\n",
       "45   UA-AC-61-1  74.707770\n",
       "46   UA-AC-65-1  73.300200\n",
       "47   UA-AD-86-1  68.761440\n",
       "48   UA-AF-81-1  67.195175\n",
       "49   UA-AF-17-1  57.880234"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/home/test/hackathon/predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f0e14-63a1-4d88-a59f-9184ed119e64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

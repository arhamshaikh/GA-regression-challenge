# Understanding Evaluation Metrics
## Multi-Center Gestational Age Prediction Challenge

**A Comprehensive Guide to Why Each Metric Matters**

---

## Table of Contents

1. [Overview: Why Multiple Metrics?](#overview)
2. [Primary Metric: Fairness-Aware Score](#primary-metric)
3. [Component Metrics](#component-metrics)
4. [Fairness Metrics](#fairness-metrics)
5. [Summary Table](#summary-table)
6. [Clinical Interpretation](#clinical-interpretation)
7. [Example Scenarios](#example-scenarios)
8. [What Good Performance Looks Like](#good-performance)

---

## 1. Overview: Why Multiple Metrics? 

### The Challenge

In medical AI, especially across multiple clinical sites, we face a critical question:

> **Is a model that's excellent at one site but terrible at another better than a model that's good everywhere?**

### The Problem with Single Metrics

**If we only used Overall MAE:**
```
Model A:
- Site 1 (Nepal, high-resource):     5 days error  âœ“âœ“âœ“
- Site 2 (Kenya, low-resource):    25 days error âœ—âœ—âœ—
- Overall MAE: 15 days

Model B:
- Site 1 (Nepal):     15 days error  âœ“
- Site 2 (Kenya):   15 days error  âœ“
- Overall MAE: 15 days
```

**Same overall MAE, but Model B is clearly better for equitable healthcare!**

### Our Solution

We use **multiple complementary metrics** that together tell the complete story:
1. **Accuracy metrics** - How well does it work overall?
2. **Fairness metrics** - Does it work consistently everywhere?
3. **Robustness metrics** - How reliable is it?

---

## 2. Primary Metric: Fairness-Aware Score 

### Definition

```
Final Score = (1 - Î») Ã— Overall_MAE + Î» Ã— Site_Disparity

where:
    Overall_MAE = Mean Absolute Error across all predictions
    Site_Disparity = max(site_MAE) - min(site_MAE)
    Î» = 0.3 (fairness weight)

Simplified:
    Final Score = 0.7 Ã— Overall_MAE + 0.3 Ã— Site_Disparity
```

**Lower is better.**

### What It Measures

A **balanced score** that considers:
- 70% weight on overall prediction accuracy
- 30% weight on consistency across sites

### Why This Is Useful for Gestational Age Prediction

#### Clinical Context
Gestational age is used to:
- Schedule prenatal visits and interventions
- Identify growth abnormalities
- Determine timing of delivery
- Plan resource allocation

**An error of 10 days might mean:**
- Missing a critical screening window
- Incorrect risk assessment
- Suboptimal timing of interventions

**But these consequences should be the same for ALL patients, regardless of where they receive care.**

#### Why Weighted Balance?

**70% on accuracy:**
- Overall performance still matters most
- Clinical utility requires good average accuracy
- Can't sacrifice too much accuracy for fairness

**30% on fairness:**
- Significant enough to matter
- Penalizes models that work well only at some sites
- Encourages developers to think about equity
- Reflects ethical imperative for equitable healthcare

### Real-World Impact

Consider deployment in a national health system with 5 regional centers:

**Model with low final score:**
- Doctors trust the system everywhere
- Consistent clinical workflows across all centers
- Equitable patient care regardless of location
- Sustainable long-term deployment

**Model with high final score (poor fairness):**
- Some centers may stop using it
- Inconsistent care quality
- Potential liability issues
- Perpetuates healthcare disparities

---

## 3. Component Metrics 

### 3.1 Overall Mean Absolute Error (MAE)

#### Definition
```
MAE = (1/N) Ã— Î£|predicted_GA - actual_GA|

where:
    N = total number of predictions
    predicted_GA = model's prediction (in days)
    actual_GA = ground truth gestational age (in days)
```

#### What It Measures

The **average prediction error** across all patients, regardless of site.

#### Units

**Days** - directly interpretable in clinical context.

#### Why This Is Useful for GA Prediction

**Clinical Interpretability:**
- MAE of 10 days = model is typically off by ~1.5 weeks
- MAE of 5 days = model is typically off by ~0.7 weeks
- MAE of 20 days = model is off by ~3 weeks

**Clinical Acceptability Thresholds:**
- **Excellent:** MAE < 7 days (~1 week) - comparable to expert sonographers
- **Good:** MAE < 10 days (~1.5 weeks) - clinically useful
- **Acceptable:** MAE < 14 days (~2 weeks) - may have limited utility
- **Poor:** MAE > 14 days - likely not clinically useful

**Why MAE over other metrics:**
- **Linear penalty:** 10-day error is exactly twice as bad as 5-day error (intuitive!)
- **Robust:** Not heavily influenced by outliers (unlike MSE/RMSE)
- **Interpretable:** Same units as the measurement (days)
- **Clinical alignment:** Matches how clinicians think about errors

#### Limitations for Multi-Site Evaluation

**What it misses:**
- A model could have excellent MAE but fail catastrophically at specific sites
- Doesn't capture consistency across different populations
- Doesn't reflect deployment feasibility across diverse settings

**Example:**
```
Sites:     [5, 7, 10, 13, 45] days
Overall MAE: 16 days

This looks "okay" but one site (45 days) is completely unNepalble!
```

---

### 3.2 Site Disparity (max_MAE - min_MAE)

#### Definition
```
Site Disparity = max(site_MAE) - min(site_MAE)

where:
    site_MAE = MAE for predictions at a specific site
    max(site_MAE) = worst-performing site
    min(site_MAE) = best-performing site
```

#### What It Measures

The **difference in performance** between the best and worst sites.

#### Units

**Days** - the gap in error between best and worst sites.

#### Why This Is Useful for GA Prediction

**Equity Assessment:**
- Disparity of 2 days = very consistent across sites âœ“
- Disparity of 5 days = moderate variation
- Disparity of 10 days = significant inequity âœ—
- Disparity of 20 days = unacceptable disparities âœ—âœ—

**Deployment Feasibility:**

Low disparity (< 5 days):
- All sites get similar quality predictions
- Can deploy with confidence everywhere
- Uniform training for clinical staff
- Consistent clinical workflows

High disparity (> 10 days):
- Some sites may reject the system
- Requires site-specific calibration
- Different confidence levels per site
- Risk of perpetuating health disparities

**Regulatory and Ethical Implications:**

Many healthcare systems and regulatory bodies are increasingly concerned with algorithmic fairness. High disparity could:
- Fail FDA approval requirements
- Violate institutional equity policies
- Expose to liability risks
- Damage trust in AI systems

**Real-World Example:**

```
Scenario: National rollout in a low-resource country

Model A: Overall MAE 12 days, Disparity 4 days
- Urban hospital: 10 days error
- Rural clinic: 14 days error
- Acceptable variation, likely deployable everywhere

Model B: Overall MAE 12 days, Disparity 18 days
- Urban hospital: 6 days error
- Rural clinic: 24 days error
- Rural sites may refuse adoption
- Perpetuates urban-rural healthcare gap
```

#### Why This Metric Alone Isn't Enough

**Limitation:** Doesn't capture overall accuracy.

```
Bad Example:
- All sites: 30 days error
- Disparity: 0 days (perfect "fairness"!)
- But clinically useless!
```

**This is why we need BOTH accuracy AND fairness in the combined score.**

---

## 4. Fairness Metrics 

### 4.1 Site Standard Deviation

#### Definition
```
site_std = âˆš[(1/n) Ã— Î£(site_MAE_i - mean_MAE)Â²]

where:
    n = number of sites
    site_MAE_i = MAE at site i
    mean_MAE = average of all site MAEs
```

#### What It Measures

The **overall spread** of performance across sites, accounting for all sites (not just best/worst).

#### Units

**Days** - standard deviation of site-level errors.

#### Why This Is Useful for GA Prediction

**More Robust Than Disparity:**

Disparity only looks at two sites (best and worst):
```
Sites: [10, 10, 10, 10, 20] days
Disparity: 10 days (only looks at 10 vs 20)
Std Dev: 4.0 days (considers all sites)
```

Sites: [10, 12, 13, 18, 20] days
Disparity: 10 days (same as above!)
Std Dev: 4.1 days (similar, but captures the gradient)
```

**Outlier Detection:**

High std dev relative to mean suggests outlier sites:
```
Model A:
- Sites: [9, 10, 10, 11, 30] days
- Mean: 14 days
- Std: 8.4 days (high relative to mean)
- â†’ Likely one problematic site

Model B:
- Sites: [12, 13, 14, 15, 16] days  
- Mean: 14 days
- Std: 1.6 days (low relative to mean)
- â†’ Consistent performance
```

**Statistical Interpretation:**

Assuming roughly normal distribution:
- ~68% of sites within Â±1 std dev of mean
- ~95% of sites within Â±2 std dev of mean

```
Example:
Mean MAE: 12 days
Std Dev: 3 days

Expected range for most sites: 9-15 days
If a site has 20+ days MAE, it's a significant outlier
```

**When Std Dev Is More Informative Than Disparity:**

1. **Many sites:** With 10+ sites, disparity might miss systematic issues
2. **Gradual degradation:** Captures gradual performance decline across sites
3. **Distribution shape:** Reveals whether issues are concentrated or widespread

#### Limitations

**What it misses:**
- Doesn't directly show worst-case performance
- Can be low even if one site is terrible (if most are good)
- Less intuitive than max-min disparity

**Example:**
```
Sites: [5, 6, 6, 7, 40] days
Std dev: 14.5 days (seems high)
But doesn't immediately show the 40-day outlier as clearly as:
Disparity: 35 days (immediately alarming!)
```

---

### 4.2 Coefficient of Variation (CV)

#### Definition
```
CV = site_std / overall_MAE

where:
    site_std = standard deviation of site MAEs
    overall_MAE = mean absolute error across all predictions
```

#### What It Measures

The **normalized variability** of performance across sites, expressed as a proportion of the overall error.

#### Units

**Dimensionless** (unitless ratio, often expressed as percentage)

#### Why This Is Useful for GA Prediction

**Allows Comparisons Across Different Scales:**

Imagine comparing two models:

```
Model A (early gestation, 60-120 days):
- Overall MAE: 5 days
- Site Std: 1 day
- CV: 0.20 (20%)

Model B (late gestation, 200-300 days):
- Overall MAE: 15 days
- Site Std: 3 days  
- CV: 0.20 (20%)
```

**Both have the same relative variability!**

Without CV, we might think Model B has worse fairness (3 days std vs 1 day), but actually both are equally consistent relative to their baseline error.

**Normalized Performance Benchmark:**

```
CV < 0.10 (10%):  Excellent consistency
CV < 0.15 (15%):  Good consistency
CV < 0.20 (20%):  Acceptable consistency
CV > 0.25 (25%):  Poor consistency
CV > 0.30 (30%):  Unacceptable variability
```

**Real-World Application:**

**Scenario 1: Comparing different GA ranges**
```
First trimester model:
- MAE: 4 days, Std: 1 day, CV: 0.25
- High relative variation for early pregnancy!

Third trimester model:
- MAE: 12 days, Std: 2 days, CV: 0.17
- Better relative consistency despite higher absolute std
```

**Scenario 2: Model improvement tracking**
```
Baseline model:
- MAE: 15 days, Std: 4.5 days, CV: 0.30

Improved model:
- MAE: 10 days, Std: 2.0 days, CV: 0.20

Not only is the model more accurate, but it's also relatively more consistent!
```

**Clinical Decision-Making:**

CV helps answer: **"Is this model's consistency good enough for my context?"**

```
Low-resource setting with high variability in equipment:
- Might accept CV < 0.25
- Consistency is hard to achieve

High-resource setting with standardized equipment:
- Should demand CV < 0.15
- No excuse for high variability
```

#### When CV Is Most Valuable

1. **Comparing models trained on different populations** with different baseline difficulties
2. **Tracking improvement** across model iterations
3. **Setting context-specific thresholds** for acceptable performance
4. **Meta-analysis** across multiple studies/datasets

#### Limitations

**Can be misleading with very low MAE:**

```
MAE: 2 days (excellent!)
Std: 1 day
CV: 0.50 (seems terrible!)

But 1 day variability is actually clinically insignificant!
```

**Solution:** Always report CV alongside absolute metrics (MAE, std dev).

**Not intuitive for clinical staff:**

Clinicians think in days, not ratios:
- "3-day difference between sites" (intuitive)
- "CV of 0.23" (requires explanation)

**Solution:** Report CV for technical evaluation, but emphasize absolute metrics for clinical communication.

---

### 4.3 Worst Site MAE

#### Definition
```
Worst Site MAE = max(site_MAE_i) for all sites i
```

#### What It Measures

The **maximum error** at any single site - the worst-case performance.

#### Units

**Days** - the MAE at the poorest-performing site.

#### Why This Is Useful for GA Prediction

**Worst-Case Clinical Utility:**

From a deployment perspective, your system is only as good as its worst site:

```
Model performance:
- Sites A, B, C, D: 8-10 days (excellent)
- Site E: 25 days (terrible)

Overall MAE: 12 days (looks okay)
Worst site: 25 days (clinically problematic!)

Outcome: Site E will likely reject the system
```

**Risk Assessment:**

Medical AI systems need worst-case guarantees:

```
Acceptable worst-case: < 15 days
- Even at worst site, clinically useful
- Can deploy system-wide with confidence

Unacceptable worst-case: > 20 days
- Risk of missed diagnoses at some sites
- May delay critical interventions
- Liability concerns
```

**Identifies Problem Sites for Targeted Improvement:**

```
All sites: [9, 10, 11, 12, 28] days

Worst site analysis reveals:
- 4 sites performing well (9-12 days)
- 1 site failing (28 days)

Action: Investigate Site 5
- Different equipment?
- Different population?
- Different operator skill level?
- Data quality issues?

This guides where to focus improvement efforts!
```

**Regulatory Requirements:**

Some regulatory frameworks require worst-case performance guarantees:
- FDA may ask: "What's your worst-case error?"
- Institutional review boards want worst-case risk assessment
- Insurance/liability considerations

**Real-World Example:**

```
National health system deployment:

Pilot Study Results:
- Average MAE: 11 days
- Worst site MAE: 23 days

Decision: NOT approved for full rollout
Rationale: Cannot accept 23-day worst-case error

After targeted improvements:
- Average MAE: 12 days (slightly worse)
- Worst site MAE: 16 days (much better)

Decision: APPROVED for full rollout
Rationale: All sites now clinically acceptable
```

#### When Worst Site MAE Is Most Important

1. **Deployment decisions** - Can we deploy everywhere?
2. **Equity assessments** - Are we failing any populations?
3. **Risk management** - What's our maximum liability?
4. **Troubleshooting** - Where should we focus improvements?

#### Limitations

**Single site may not be representative:**
- Could be due to small sample size
- May be an outlier in data quality
- Might not reflect typical performance

**Doesn't show how many sites are problematic:**
```
Scenario A: [10, 10, 10, 10, 25] days
- Worst: 25, but only 1 site bad

Scenario B: [15, 18, 20, 22, 25] days
- Worst: 25, but ALL sites are problematic!

Disparity and std dev help distinguish these cases.
```

---

### 4.4 Best Site MAE

#### Definition
```
Best Site MAE = min(site_MAE_i) for all sites i
```

#### What It Measures

The **minimum error** at any single site - the best-case performance.

#### Units

**Days** - the MAE at the best-performing site.

#### Why This Is Useful for GA Prediction

**Performance Ceiling:**

Best site shows what your model is **theoretically capable of** under optimal conditions:

```
Best site: 6 days
Worst site: 18 days

Insight: Model CAN achieve 6-day accuracy!
Question: Why doesn't it achieve this everywhere?
```

**Fairness Analysis:**

Comparison with worst site reveals equity gap:

```
Model A:
- Best: 8 days, Worst: 10 days
- Small gap (2 days) = good fairness

Model B:
- Best: 6 days, Worst: 20 days
- Large gap (14 days) = poor fairness
```

**Site-Specific Issues:**

If best site is still poor, it's a fundamental model problem:
```
Best site: 25 days
Worst site: 35 days

Problem: Even under best conditions, model is inadequate!
Not a fairness issue, it's an accuracy issue.
```

**Optimization Target:**

Best site performance sets a benchmark:
```
Current:
- Best: 7 days (what IS achievable)
- Average: 12 days
- Worst: 18 days

Goal: Bring average and worst closer to 7 days
Strategy: Study what makes best site perform well
- Better data quality?
- Different population characteristics?
- Equipment differences?
```

#### When Best Site MAE Is Most Informative

1. **Root cause analysis** - Is poor performance fundamental or site-specific?
2. **Improvement potential** - How much can we realistically improve?
3. **Resource allocation** - Should we improve the model or focus on data quality?

#### Limitations

**Best site may not be representative:**
- Could be easier population
- Better equipment
- More experienced operators
- Lucky sample

**Limited actionable insight alone:**
- Knowing best performance doesn't directly tell you how to achieve it everywhere

**Need to combine with other metrics:**
```
Best: 5 days
Worst: 6 days
Disparity: 1 day (excellent!)

vs.

Best: 5 days
Worst: 25 days
Disparity: 20 days (terrible!)

Both have the same "best site" but very different stories!
```

---

### 4.5 Per-Site Breakdown

#### Definition

Individual MAE calculated for each site:
```
Site A: 8.3 days
Site B: 9.8 days
Site C: 10.5 days
Site D: 11.2 days
Site E: 12.1 days
```

#### What It Measures

**Detailed performance profile** showing how well the model works at each specific location.

#### Why This Is Useful for GA Prediction

**Granular Performance Assessment:**

Reveals the complete distribution of performance:

```
Scenario 1 - Gradual degradation:
[8, 9, 10, 11, 12] days
â†’ Systematic issue across sites (equipment quality gradient?)

Scenario 2 - Bimodal distribution:
[8, 8, 9, 18, 19] days
â†’ Two populations (high-resource vs low-resource?)

Scenario 3 - Single outlier:
[9, 10, 10, 11, 25] days
â†’ One problematic site (investigate specifically)
```


#### When Per-Site Breakdown Is Essential

1. **Troubleshooting** - Where and why is the model failing?
2. **Improvement prioritization** - Which sites need attention first?
3. **Deployment planning** - Which sites are ready for deployment?
4. **Fairness auditing** - Are there systematic biases?
5. **Stakeholder reporting** - Site-specific communication


---

## 5. Summary Table {#summary-table}

| Metric | Definition | Units | What It Measures | Why It Matters for GA | Good Value | Poor Value |
|--------|-----------|-------|------------------|----------------------|------------|------------|
| **Overall MAE** | Mean absolute error | Days | Average prediction accuracy | Clinical utility baseline | < 10 days | > 15 days |
| **Site Disparity** | max_MAE - min_MAE | Days | Gap between best/worst sites | Equity and deployment feasibility | < 5 days | > 10 days |
| **Final Score** | 0.7Ã—MAE + 0.3Ã—Disparity | Days | Balanced accuracy + fairness | Primary ranking metric | < 12 days | > 18 days |
| **Site Std Dev** | Std of site MAEs | Days | Overall spread across sites | Robustness assessment | < 2 days | > 4 days |
| **Coefficient of Variation** | Std / MAE | Unitless | Normalized consistency | Scale-independent fairness | < 0.15 | > 0.25 |
| **Worst Site MAE** | Maximum site MAE | Days | Worst-case performance | Risk assessment | < 15 days | > 20 days |
| **Best Site MAE** | Minimum site MAE | Days | Best-case performance | Performance ceiling | < 8 days | > 12 days |
| **Per-Site MAEs** | Individual site MAEs | Days | Detailed profile | Troubleshooting & planning | All < 12 | Any > 18 |

---

## 6. Clinical Interpretation 

### How to Interpret Combined Metrics

#### Scenario A: Excellent Model
```
Overall MAE:     9.2 days       âœ“âœ“âœ“
Site Disparity:  2.8 days       âœ“âœ“âœ“
Final Score:     9.28 days      âœ“âœ“âœ“
Site Std Dev:    1.1 days       âœ“âœ“âœ“
CV:              0.12           âœ“âœ“âœ“
Worst Site:      10.6 days      âœ“âœ“
Best Site:       7.8 days       âœ“âœ“âœ“
Per-Site:        [7.8, 8.9, 9.5, 10.1, 10.6]

Interpretation:
- Excellent accuracy (~1.3 weeks average error)
- Very consistent across sites (2.8 days disparity)
- Low relative variability (CV = 12%)
- Even worst site is clinically useful
- Ready for deployment at ALL sites

Clinical Use:
- Can deploy system-wide immediately
- Set uniform workflows across all sites
- High confidence for all patient populations
```

#### Scenario B: Good Accuracy, Poor Fairness
```
Overall MAE:     10.5 days      âœ“âœ“
Site Disparity:  12.3 days      âœ—
Final Score:     11.04 days     âœ—
Site Std Dev:    4.8 days       âœ—
CV:              0.46           âœ—âœ—
Worst Site:      18.9 days      âœ—
Best Site:       6.6 days       âœ“âœ“âœ“
Per-Site:        [6.6, 9.2, 11.4, 15.1, 18.9]

Interpretation:
- Good average accuracy
- But HUGE variation between sites
- Model works great at some sites, fails at others
- High CV indicates severe inconsistency
- Worst site performance unacceptable

Clinical Use:
- Deploy ONLY at sites A, B, C
- Hold back from sites D, E
- Investigate why such large variation
- Targeted improvement needed

Root Cause Investigation:
- Check equipment differences (Sites A vs E)
- Check population differences
- Check operator training levels
- Check data quality
```

#### Scenario C: Poor Overall, But Fair
```
Overall MAE:     18.2 days      âœ—
Site Disparity:  3.1 days       âœ“âœ“âœ“
Final Score:     18.67 days     âœ—âœ—
Site Std Dev:    1.3 days       âœ“âœ“âœ“
CV:              0.07           âœ“âœ“âœ“
Worst Site:      19.8 days      âœ—
Best Site:       16.7 days      âœ—
Per-Site:        [16.7, 17.9, 18.1, 19.2, 19.8]

Interpretation:
- Poor accuracy across ALL sites
- But at least it's consistently poor
- Not a fairness problem, it's an accuracy problem
- Excellent consistency (CV = 7%)
- Fundamental model limitation

Clinical Use:
- NOT ready for clinical deployment anywhere
- Need to improve fundamental model accuracy
- At least no equity concerns
- Once improved, should work well everywhere

Action Items:
- Improve model architecture
- Collect more/better training data
- Try different training strategies
- NOT a site-specific issue
```

#### Scenario D: Moderate Everything
```
Overall MAE:     12.8 days      âœ“
Site Disparity:  6.2 days       âœ—
Final Score:     13.82 days     âœ—
Site Std Dev:    2.4 days       â—‹
CV:              0.19           â—‹
Worst Site:      15.6 days      â—‹
Best Site:       9.4 days       âœ“
Per-Site:        [9.4, 11.8, 12.9, 14.1, 15.6]

Interpretation:
- Acceptable but not great accuracy
- Moderate fairness concerns
- Gradual degradation across sites
- All metrics in "okay" range
- Might be acceptable depending on context

Clinical Use:
- Borderline for deployment
- Might be acceptable in low-resource settings
- Would benefit from improvement
- Could deploy with careful monitoring

Decision Factors:
- What's the alternative? (Manual GA estimation)
- Resource constraints?
- Patient population needs?
- Improvement timeline?
```

---


### Red Flags to Watch For

ðŸš© **High Disparity with Good Average:**
- Model may be failing specific populations
- Equity concern
- Investigation needed

ðŸš© **Very High CV (>0.30):**
- Inconsistent performance
- May indicate fundamental generalization problems
- Training data imbalance likely

ðŸš© **Worst Site > 20 days:**
- Unacceptable worst-case performance
- Liability and safety concerns
- Block deployment

ðŸš© **Best Site < 7 days but Average > 14 days:**
- Model CAN perform well but isn't everywhere
- Suggests fixable site-specific issues
- Good candidate for targeted improvements

ðŸš© **All Sites Mediocre (13-17 days):**
- Fundamental model limitation
- Not a fairness issue
- Need better architecture/data/training


---

## 7. Putting It All Together

### The Complete Story

**Single metrics only tell part of the story. Together, they reveal:**

1. **Overall MAE** â†’ Is it accurate enough to be useful?
2. **Site Disparity** â†’ Does it work consistently everywhere?
3. **Final Score** â†’ Balanced ranking considering both
4. **Site Std Dev** â†’ How spread out is performance?
5. **CV** â†’ Is variability appropriate for this scale?
6. **Worst Site** â†’ What's our worst-case risk?
7. **Best Site** â†’ What's our best-case ceiling?
8. **Per-Site** â†’ Where exactly are the problems?

### Key Takeaways

âœ… **For Competition Participants:**
- Optimize for Final Score (accuracy + fairness)
- Monitor all metrics, not just MAE
- Investigate high CV or disparity
- Don't sacrifice fairness for marginal MAE gains

âœ… **For Clinicians:**
- Look at Overall MAE for average clinical utility
- Look at Worst Site for risk assessment
- Look at Disparity for equity concerns
- Check Per-Site for your specific location


- Report ALL metrics for complete picture
- Use CV for cross-study comparisons
- Analyze Per-Site for insights
- Don't cherry-pick favorable metrics

---

## Conclusion

Multiple metrics exist because **medical AI is complex**:
- It must be accurate
- It must be fair
- It must be reliable
- It must work everywhere

By evaluating models through this comprehensive lens, we ensure that deployed systems:
- âœ… Work well for ALL patients
- âœ… Don't perpetuate healthcare disparities  
- âœ… Are trustworthy across diverse settings
- âœ… Have acceptable worst-case performance

**The goal isn't just a good averageâ€”it's good, equitable healthcare for everyone.**

---

**Questions? See the main competition README or contact organizers.**

---

**Document Version:** 1.0  
**Last Updated:** [17.11.2025]  
**Author:** Competition Organizers
